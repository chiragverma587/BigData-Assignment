{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88705c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d9f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_hadoop_components(config_file):\n",
    "    with open(config_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        core_components = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('<property>'):\n",
    "                component = None\n",
    "            elif line.startswith('<name>'):\n",
    "                component = line.split('>')[1].split('<')[0]\n",
    "            elif line.startswith('<value>'):\n",
    "                value = line.split('>')[1].split('<')[0]\n",
    "                if component == 'fs.defaultFS':\n",
    "                    core_components.append(value)\n",
    "        \n",
    "        print(\"Core Components of Hadoop:\")\n",
    "        for component in core_components:\n",
    "            print(component)\n",
    "\n",
    "# Specify the path to your Hadoop configuration file\n",
    "config_file = 'C:/hadoop/conf/hdfs-site.xml'\n",
    "\n",
    "# Call the function to display the core components\n",
    "display_hadoop_components(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8964e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "def calculate_directory_size(hdfs_url, directory_path):\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    # Function to calculate the size of a file or directory\n",
    "    def get_size(path):\n",
    "        if client.status(path)['type'] == 'FILE':\n",
    "            return client.status(path)['length']\n",
    "        else:\n",
    "            total_size = 0\n",
    "            for content in client.list(path):\n",
    "                total_size += get_size(os.path.join(path, content['pathSuffix']))\n",
    "            return total_size\n",
    "\n",
    "    # Calculate the total size of the directory\n",
    "    total_size = get_size(directory_path)\n",
    "    return total_size\n",
    "\n",
    "# Specify the HDFS URL and the directory path\n",
    "hdfs_url = 'http://localhost:50070'\n",
    "directory_path = '/user/hadoop/data'\n",
    "\n",
    "# Call the function to calculate the directory size\n",
    "directory_size = calculate_directory_size(hdfs_url, directory_path)\n",
    "\n",
    "print(f\"Total size of directory '{directory_path}' in HDFS: {directory_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9389126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bae70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "def mapper(text):\n",
    "    # Emit key-value pairs (word, 1) for each word in the text\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        yield (word, 1)\n",
    "\n",
    "def reducer(word, counts):\n",
    "    # Sum up the counts for each word and emit (word, total_count)\n",
    "    yield (word, sum(counts))\n",
    "\n",
    "def top_n_frequent_words(file_path, n):\n",
    "    # Initialize a dictionary to store word counts\n",
    "    word_counts = defaultdict(int)\n",
    "\n",
    "    # Open the file and process each line\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Map phase: process each line using the mapper function\n",
    "            for word, count in mapper(line):\n",
    "                # Aggregate the counts for each word\n",
    "                word_counts[word] += count\n",
    "\n",
    "    # Reduce phase: process the word counts using the reducer function\n",
    "    reduced_counts = []\n",
    "    for word, count in word_counts.items():\n",
    "        for word, reduced_count in reducer(word, [count]):\n",
    "            reduced_counts.append((word, reduced_count))\n",
    "\n",
    "    # Sort the word counts in descending order\n",
    "    sorted_counts = sorted(reduced_counts, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Select the top N words\n",
    "    top_n = heapq.nlargest(n, sorted_counts, key=lambda x: x[1])\n",
    "\n",
    "    # Display the top N words and their counts\n",
    "    for word, count in top_n:\n",
    "        print(f\"Word: {word}\\tCount: {count}\")\n",
    "\n",
    "# Provide the file path and the value of N\n",
    "file_path = '/user/hadoop/input/file.txt'\n",
    "N = 10\n",
    "\n",
    "# Call the function to display the top N frequent words\n",
    "top_n_frequent_words(file_path, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def check_hadoop_cluster_health(nn_host):\n",
    "    # Check NameNode health status\n",
    "    nn_health_url = f\"http://{nn_host}:50070/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo\"\n",
    "    try:\n",
    "        nn_response = requests.get(nn_health_url)\n",
    "        nn_data = json.loads(nn_response.text)\n",
    "        nn_status = nn_data['beans'][0]['State']\n",
    "        print(f\"NameNode Status: {nn_status}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing NameNode: {e}\")\n",
    "\n",
    "    # Check DataNode health status\n",
    "    dn_health_url = f\"http://{nn_host}:50070/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState-null\"\n",
    "    try:\n",
    "        dn_response = requests.get(dn_health_url)\n",
    "        dn_data = json.loads(dn_response.text)\n",
    "        dn_status = dn_data['beans'][0]['VolumeFailuresTotal']\n",
    "        print(f\"DataNode Status: {dn_status} volume failure(s)\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing DataNodes: {e}\")\n",
    "\n",
    "# Specify the NameNode host\n",
    "nn_host = 'localhost'\n",
    "\n",
    "# Call the function to check Hadoop cluster health\n",
    "check_hadoop_cluster_health(nn_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Develop a Python program that lists all the files and directories in a specific HDFS path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def list_hdfs_path(hdfs_url, hdfs_path):\n",
    "    client = InsecureClient(hdfs_url)\n",
    "    \n",
    "    # List all the files and directories in the given HDFS path\n",
    "    contents = client.list(hdfs_path, status=True)\n",
    "    \n",
    "    print(f\"Contents of HDFS path '{hdfs_path}':\")\n",
    "    for content in contents:\n",
    "        # Get the path and type (file or directory) of each content\n",
    "        path = content['path']\n",
    "        content_type = content['type']\n",
    "        \n",
    "        # Print the path and type\n",
    "        print(f\"{path}\\t\\t{content_type}\")\n",
    "\n",
    "# Specify the HDFS URL and the HDFS path to list\n",
    "hdfs_url = 'http://localhost:50070'\n",
    "hdfs_path = '/user/hadoop/data'\n",
    "\n",
    "# Call the function to list the contents of the HDFS path\n",
    "list_hdfs_path(hdfs_url, hdfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef666f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def analyze_storage_utilization(hdfs_url):\n",
    "    client = InsecureClient(hdfs_url)\n",
    "    \n",
    "    # Get the DataNode information\n",
    "    datanode_info = client.get_datanode_report()\n",
    "    \n",
    "    # Extract storage utilization information from the DataNode report\n",
    "    storage_utilization = {}\n",
    "    for datanode in datanode_info['beans']:\n",
    "        node_name = datanode['name'].replace(\"DataNodeInfo\", \"\")\n",
    "        used_storage = datanode['used']\n",
    "        capacity = datanode['capacity']\n",
    "        storage_utilization[node_name] = (used_storage, capacity)\n",
    "    \n",
    "    # Identify the DataNode with the highest and lowest storage capacities\n",
    "    highest_capacity_node = max(storage_utilization, key=lambda x: storage_utilization[x][1])\n",
    "    lowest_capacity_node = min(storage_utilization, key=lambda x: storage_utilization[x][1])\n",
    "    \n",
    "    # Display the storage utilization information\n",
    "    print(\"Storage Utilization:\")\n",
    "    for node, (used, capacity) in storage_utilization.items():\n",
    "        utilization_percentage = (used / capacity) * 100\n",
    "        print(f\"{node}\\tUsed: {used} bytes\\tCapacity: {capacity} bytes\\tUtilization: {utilization_percentage:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nNode with highest capacity: {highest_capacity_node}\")\n",
    "    print(f\"Node with lowest capacity: {lowest_capacity_node}\")\n",
    "\n",
    "# Specify the HDFS URL\n",
    "hdfs_url = 'http://localhost:50070'\n",
    "\n",
    "# Call the function to analyze storage utilization\n",
    "analyze_storage_utilization(hdfs_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e34f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6af120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(yarn_url, job_conf_file):\n",
    "    # Submit the Hadoop job to YARN ResourceManager\n",
    "    submit_url = f\"{yarn_url}/ws/v1/cluster/apps/new-application\"\n",
    "    try:\n",
    "        response = requests.post(submit_url)\n",
    "        data = json.loads(response.text)\n",
    "        application_id = data['application-id']\n",
    "        print(f\"Application ID: {application_id}\")\n",
    "\n",
    "        # Upload the job configuration file to HDFS\n",
    "        upload_url = f\"{yarn_url}/ws/v1/cluster/apps/{application_id}/upload\"\n",
    "        with open(job_conf_file, 'rb') as file:\n",
    "            upload_response = requests.post(upload_url, files={'file': file})\n",
    "\n",
    "        # Submit the job to ResourceManager\n",
    "        job_submit_url = f\"{yarn_url}/ws/v1/cluster/apps\"\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        job_submit_payload = {\n",
    "            \"application-id\": application_id,\n",
    "            \"application-name\": \"HadoopJob\",\n",
    "            \"am-container-spec\": {\n",
    "                \"commands\": {\n",
    "                    \"command\": f\"hadoop jar {job_conf_file} [YOUR_JOB_ARGS]\"\n",
    "                }\n",
    "            },\n",
    "            \"application-type\": \"MAPREDUCE\"\n",
    "        }\n",
    "        submit_response = requests.post(job_submit_url, headers=headers, json=job_submit_payload)\n",
    "        print(\"Hadoop job submitted successfully!\")\n",
    "        \n",
    "        # Return the application ID\n",
    "        return application_id\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error submitting Hadoop job: {e}\")\n",
    "\n",
    "def monitor_job_progress(yarn_url, application_id):\n",
    "    # Monitor the progress of the Hadoop job\n",
    "    status_url = f\"{yarn_url}/ws/v1/cluster/apps/{application_id}\"\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(status_url)\n",
    "            data = json.loads(response.text)\n",
    "            state = data['app']['state']\n",
    "            final_status = data['app']['finalStatus']\n",
    "\n",
    "            if state == 'FINISHED' and final_status == 'SUCCEEDED':\n",
    "                print(\"Hadoop job completed successfully!\")\n",
    "                break\n",
    "            elif state == 'FINISHED' and final_status == 'FAILED':\n",
    "                print(\"Hadoop job failed!\")\n",
    "                break\n",
    "            elif state == 'FINISHED' and final_status == 'KILLED':\n",
    "                print(\"Hadoop job was killed!\")\n",
    "                break\n",
    "\n",
    "            print(f\"Job state: {state}\\tFinal status: {final_status}\")\n",
    "\n",
    "            time.sleep(5)  # Wait for 5 seconds before checking again\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error monitoring job progress: {e}\")\n",
    "            break\n",
    "\n",
    "def retrieve_job_output(yarn_url, application_id):\n",
    "    # Retrieve the final output of the Hadoop job\n",
    "    logs_url = f\"{yarn_url}/proxy/{application_id}/ws/v1/mapreduce/jobs\"\n",
    "    try:\n",
    "        response = requests.get(logs_url)\n",
    "        data = json.loads(response.text)\n",
    "        job_id = data['jobs']['job'][0]['id']\n",
    "        \n",
    "        # Retrieve job logs\n",
    "        logs_url = f\"{yarn_url}/proxy/{application_id}/ws/v1/mapreduce/jobs/{job_id}/jobattempts\"\n",
    "        logs_response = requests.get(logs_url)\n",
    "        logs_data = json.loads(logs_response.text)\n",
    "        logs = logs_data['jobAttempts']['jobAttempt'][0]['logsLink']\n",
    "        \n",
    "        # Print the logs URL\n",
    "        print(f\"Job Logs: {logs}\")\n",
    "        \n",
    "        # You can further process the logs or download the output from HDFS\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error retrieving job output: {e}\")\n",
    "\n",
    "# Specify the YARN ResourceManager URL and the path to the job configuration file\n",
    "yarn_url = 'http://localhost:8088'\n",
    "job_conf_file = 'C:/hadoop/conf/hdfs-site.xml'\n",
    "\n",
    "# Submit the Hadoop job\n",
    "application_id = submit_hadoop_job(yarn_url, job_conf_file)\n",
    "\n",
    "# Monitor job progress until completion\n",
    "monitor_job_progress(yarn_url, application_id)\n",
    "\n",
    "# Retrieve the final output of the job\n",
    "retrieve_job_output(yarn_url, application_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81441925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(yarn_url, job_conf_file, resource_memory, resource_vcores):\n",
    "    # Submit the Hadoop job to YARN ResourceManager\n",
    "    submit_url = f\"http://resourcemanager.example.com:8088/ws/v1/cluster/apps/new-application\"\n",
    "    try:\n",
    "        response = requests.post(submit_url)\n",
    "        data = json.loads(response.text)\n",
    "        application_id = data['application-id']\n",
    "        print(f\"Application ID: {application_id}\")\n",
    "\n",
    "        # Upload the job configuration file to HDFS\n",
    "        upload_url = f\"http://resourcemanager.example.com:8088/ws/v1/cluster/apps/{application_id}/upload\"\n",
    "        with open(job_conf_file, 'rb') as file:\n",
    "            upload_response = requests.post(upload_url, files={'file': file})\n",
    "\n",
    "        # Set resource requirements for the job\n",
    "        resource_url = f\"http://resourcemanager.example.com:8088/ws/v1/cluster/apps/{application_id}/resource\"\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        resource_payload = {\n",
    "            \"resource\": {\n",
    "                \"memory\": resource_memory,\n",
    "                \"vCores\": resource_vcores\n",
    "            }\n",
    "        }\n",
    "        resource_response = requests.put(resource_url, headers=headers, json=resource_payload)\n",
    "        print(f\"Resource requirements set: memory={resource_memory}, vCores={resource_vcores}\")\n",
    "\n",
    "        # Submit the job to ResourceManager\n",
    "        job_submit_url = f\"{yarn_url}/ws/v1/cluster/apps\"\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        job_submit_payload = {\n",
    "            \"application-id\": application_id,\n",
    "            \"application-name\": \"HadoopJob\",\n",
    "            \"am-container-spec\": {\n",
    "                \"commands\": {\n",
    "                    \"command\": f\"hadoop jar {job_conf_file} [YOUR_JOB_ARGS]\"\n",
    "                }\n",
    "            },\n",
    "            \"application-type\": \"MAPREDUCE\"\n",
    "        }\n",
    "        submit_response = requests.post(job_submit_url, headers=headers, json=job_submit_payload)\n",
    "        print(\"Hadoop job submitted successfully!\")\n",
    "\n",
    "        # Return the application ID\n",
    "        return application_id\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error submitting Hadoop job: {e}\")\n",
    "\n",
    "def track_resource_usage(yarn_url, application_id):\n",
    "    # Track the resource usage of the Hadoop job\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(f\"{yarn_url}/ws/v1/cluster/apps/{application_id}\")\n",
    "            data = json.loads(response.text)\n",
    "            state = data['app']['state']\n",
    "            final_status = data['app']['finalStatus']\n",
    "            allocated_resources = data['app']['allocatedResources']\n",
    "            progress = data['app']['progress']\n",
    "\n",
    "            print(f\"Job state: {state}\\tFinal status: {final_status}\")\n",
    "            print(f\"Progress: {progress}%\")\n",
    "\n",
    "            if state == 'FINISHED' and final_status == 'SUCCEEDED':\n",
    "                print(\"Hadoop job completed successfully!\")\n",
    "                break\n",
    "            elif state == 'FINISHED' and final_status == 'FAILED':\n",
    "                print(\"Hadoop job failed!\")\n",
    "                break\n",
    "            elif state == 'FINISHED' and final_status == 'KILLED':\n",
    "                print(\"Hadoop job was killed!\")\n",
    "                break\n",
    "\n",
    "            if allocated_resources is not None:\n",
    "                memory = allocated_resources['memory']\n",
    "                vcores = allocated_resources['vCores']\n",
    "                print(f\"Allocated resources: memory={memory}, vCores={vcores}\")\n",
    "\n",
    "            time.sleep(5)  # Wait for 5 seconds before checking again\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error tracking resource usage: {e}\")\n",
    "            break\n",
    "\n",
    "# Specify the YARN ResourceManager URL, path to the job configuration file,\n",
    "# and the resource requirements for the job\n",
    "yarn_url = 'http://resourcemanager.example.com:8088'\n",
    "job_conf_file = '/user/hadoop/jobs/wordcount/conf.xml'\n",
    "resource_memory = 2048  # in MB\n",
    "resource_vcores = 2\n",
    "\n",
    "# Submit the Hadoop job\n",
    "application_id = submit_hadoop_job(yarn_url, job_conf_file, resource_memory, resource_vcores)\n",
    "\n",
    "# Track resource usage during job execution\n",
    "track_resource_usage(yarn_url, application_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a11be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba7865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_mapreduce_job(input_path, output_path, split_size):\n",
    "    # Clear output directory before running the job\n",
    "    subprocess.run(['hadoop', 'fs', '-rm', '-r', '-f', output_path])\n",
    "\n",
    "    # Set the input split size\n",
    "    subprocess.run(['hadoop', 'fs', '-D', 'mapreduce.input.fileinputformat.split.maxsize=' + split_size,\n",
    "                    '-D', 'mapreduce.input.fileinputformat.split.minsize=' + split_size,\n",
    "                    '-put', input_path, input_path])\n",
    "\n",
    "    # Run the MapReduce job and measure execution time\n",
    "    start_time = time.time()\n",
    "    subprocess.run(['hadoop', 'jar', 'your_job_jar_file.jar', 'YourJobClass', input_path, output_path])\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Print execution time for the job\n",
    "    print(f\"Execution time with split size {split_size}: {execution_time} seconds\")\n",
    "\n",
    "# Specify the input and output paths for the MapReduce job\n",
    "input_path = '/user/hadoop/input'\n",
    "output_path = '/user/hadoop/output'\n",
    "\n",
    "# Specify different input split sizes to compare\n",
    "split_sizes = ['64', '128', '256']\n",
    "\n",
    "# Run the MapReduce job with different split sizes and compare execution times\n",
    "for split_size in split_sizes:\n",
    "    run_mapreduce_job(input_path, output_path, split_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
