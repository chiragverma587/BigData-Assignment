{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Working with RDDs:\n",
    "#    a) Write a Python program to create an RDD from a local data source.\n",
    "#    b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
    "#    c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"spark1\")\n",
    "\n",
    "# Create an RDD from a local data source\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Perform transformations and actions on the RDD\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)  # Square each element\n",
    "filtered_rdd = squared_rdd.filter(lambda x: x > 10)  # Filter elements greater than 10\n",
    "sum_of_elements = filtered_rdd.reduce(lambda x, y: x + y)  # Reduce to calculate the sum\n",
    "\n",
    "# Print the RDD and the result\n",
    "print(\"Original RDD:\")\n",
    "for element in rdd.collect():\n",
    "    print(element)\n",
    "\n",
    "print(\"\\nTransformed RDD:\")\n",
    "for element in squared_rdd.collect():\n",
    "    print(element)\n",
    "\n",
    "print(\"\\nFiltered RDD:\")\n",
    "for element in filtered_rdd.collect():\n",
    "    print(element)\n",
    "\n",
    "print(\"\\nSum of elements:\", sum_of_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a1ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Spark DataFrame Operations:\n",
    "#    a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
    "#    b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
    "#    c) Apply Spark SQL queries on the DataFrame to extract insights from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a37997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"spark2\").getOrCreate()\n",
    "\n",
    "# Load a CSV file into a DataFrame\n",
    "df = spark.read.csv(\"C:\\Users\\asus\\Downloads/employee.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display the DataFrame schema\n",
    "df.printSchema()\n",
    "\n",
    "# Perform filtering\n",
    "filtered_df = df.filter(df[\"age\"] > 25)\n",
    "\n",
    "# Perform grouping and aggregation\n",
    "grouped_df = df.groupBy(\"gender\").agg({\"salary\": \"avg\"})\n",
    "\n",
    "# Perform joining with another DataFrame\n",
    "other_df = spark.read.csv(\"C:\\Users\\asus\\Downloads/department.csv\", header=True, inferSchema=True)\n",
    "joined_df = df.join(other_df, \"deptid\")\n",
    "\n",
    "# Apply Spark SQL queries on the DataFrame\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "result = spark.sql(\"SELECT * FROM employees WHERE salary > 50000\")\n",
    "\n",
    "# Display the results\n",
    "filtered_df.show()\n",
    "grouped_df.show()\n",
    "joined_df.show()\n",
    "result.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Spark Streaming:\n",
    "#   a) Write a Python program to create a Spark Streaming application.\n",
    "#    b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).\n",
    "#    c) Implement streaming transformations and actions to process and analyze the incoming data stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c460b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a StreamingContext with a batch interval of 1 second\n",
    "ssc = StreamingContext(sparkContext, 1)\n",
    "\n",
    "# Configure the application to consume data from a socket\n",
    "hostname = \"localhost\"\n",
    "port = 4041\n",
    "lines = ssc.socketTextStream(hostname, port)\n",
    "\n",
    "# Implement streaming transformations and actions\n",
    "keywords = [\"error\", \"warning\", \"critical\"]\n",
    "\n",
    "# Count occurrences of keywords in the streaming data\n",
    "keyword_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "                     .filter(lambda word: word.lower() in keywords) \\\n",
    "                     .map(lambda word: (word.lower(), 1)) \\\n",
    "                     .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Print the keyword counts\n",
    "keyword_counts.pprint()\n",
    "\n",
    "# Start the streaming computation\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the streaming computation to finish\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e8dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Spark SQL and Data Source Integration:\n",
    "#    a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).\n",
    "#    b)Perform SQL operations on the data stored in the database using Spark SQL.\n",
    "#    c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c67d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark4\").getOrCreate()\n",
    "\n",
    "# Connect Spark with a relational database (e.g., MySQL, PostgreSQL)\n",
    "db_url = \"jdbc:postgresql://localhost:5432/mydatabase\"\n",
    "db_properties = {\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mypassword\"\n",
    "}\n",
    "\n",
    "# Load data from a database table into a DataFrame\n",
    "table_name = \"employee\"\n",
    "df = spark.read.jdbc(url=db_url, table=table_name, properties=db_properties)\n",
    "\n",
    "# Perform SQL operations on the data stored in the database\n",
    "df.createOrReplaceTempView(\"employee1\")\n",
    "\n",
    "# Execute SQL queries on the DataFrame\n",
    "result = spark.sql(\"SELECT * FROM employee1 WHERE empid > 10\")\n",
    "\n",
    "# Display the result\n",
    "result.show()\n",
    "\n",
    "# Explore integration with other data sources\n",
    "# Read data from HDFS\n",
    "hdfs_path = \"hdfs://localhost:9000/data/employee.csv\"\n",
    "df_hdfs = spark.read.csv(hdfs_path, header=True, inferSchema=True)\n",
    "\n",
    "# Read data from Amazon S3\n",
    "s3_path = \"s3a://my-bucket/data/employee.csv\"\n",
    "df_s3 = spark.read.csv(s3_path, header=True, inferSchema=True)\n",
    "\n",
    "# Perform operations on the data from other data sources\n",
    "# Example: Count the number of records\n",
    "count_hdfs = df_hdfs.count()\n",
    "count_s3 = df_s3.count()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Count from HDFS:\", count_hdfs)\n",
    "print(\"Count from S3:\", count_s3)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
