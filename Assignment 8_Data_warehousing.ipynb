{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e3c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Design a data warehouse schema for a retail company that includes dimension tables for products, customers, and time. Implement the schema using a relational database management system (RDBMS) of your choice.\n",
    "\n",
    "Schema Design:\n",
    "Product Dimension Table:\n",
    "    product_id (Primary Key)\n",
    "    product_name\n",
    "    category\n",
    "    brand\n",
    "    price\n",
    "Customer Dimension Table:\n",
    "    customer_id (Primary Key)\n",
    "    customer_name\n",
    "    address\n",
    "    email\n",
    "Time Dimension Table:\n",
    "    date_id (Primary Key)\n",
    "    date\n",
    "    day_of_week\n",
    "    month\n",
    "    quarter\n",
    "    year\n",
    "Sales Fact Table:\n",
    "    sales_id (Primary Key)\n",
    "    product_id (Foreign Key referencing Product Dimension Table)\n",
    "    customer_id (Foreign Key referencing Customer Dimension Table)\n",
    "    date_id (Foreign Key referencing Time Dimension Table)\n",
    "    quantity\n",
    "    revenue\n",
    "\n",
    "\n",
    "\n",
    "CREATE TABLE ProductDimension (\n",
    "    product_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    product_name VARCHAR(255),\n",
    "    category VARCHAR(50),\n",
    "    brand VARCHAR(50),\n",
    "    price DECIMAL(10, 2)\n",
    ");\n",
    "\n",
    "CREATE TABLE CustomerDimension (\n",
    "    customer_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    customer_name VARCHAR(255),\n",
    "    address VARCHAR(255),\n",
    "    email VARCHAR(255)\n",
    ");\n",
    "\n",
    "CREATE TABLE TimeDimension (\n",
    "    date_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    date DATE,\n",
    "    day_of_week VARCHAR(20),\n",
    "    month VARCHAR(20),\n",
    "    quarter VARCHAR(20),\n",
    "    year INT);\n",
    "\n",
    "CREATE TABLE SalesFact (\n",
    "    sales_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    product_id INT FOREIGN KEY REFERENCES ProductDimension(product_id),\n",
    "    customer_id INT FOREIGN KEY REFERENCES CustomerDimension(customer_id),\n",
    "    date_id INT FOREIGN KEY REFERENCES TimeDimension(date_id),\n",
    "    quantity INT,\n",
    "    revenue DECIMAL(10, 2)\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Create a fact table that captures sales data, including product ID, customer ID, date, and sales amount. Populate the fact table with sample data.\n",
    "\n",
    "CREATE TABLE Sales (\n",
    "  SaleID INT PRIMARY KEY,\n",
    "  ProductID INT,\n",
    "  CustomerID INT,\n",
    "  SaleDate DATE,\n",
    "  SaleAmount DECIMAL(10, 2),\n",
    "  -- Add other relevant columns for sales details\n",
    ");\n",
    "\n",
    "-- Insert sample data into the Sales fact table\n",
    "INSERT INTO Sales (SaleID, ProductID, CustomerID, SaleDate, SaleAmount)\n",
    "VALUES\n",
    "  (1, 101, 201, '2023-07-01', 100.50),\n",
    "  (2, 102, 202, '2023-07-02', 75.20),\n",
    "  (3, 103, 201, '2023-07-03', 150.80),\n",
    "  (4, 101, 203, '2023-07-04', 80.90),\n",
    "  -- Add more sample records as needed\n",
    "  ;\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a6b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Write SQL queries to retrieve sales data from the data warehouse, including aggregations and filtering based on different dimensions.\n",
    "\n",
    "Retrieve total revenue for each product:\n",
    "SELECT\n",
    "    p.product_id,\n",
    "    p.product_name,\n",
    "    SUM(s.revenue) AS total_revenue\n",
    "FROM\n",
    "    SalesFact s\n",
    "    INNER JOIN ProductDimension p ON s.product_id = p.product_id\n",
    "GROUP BY\n",
    "    p.product_id, p.product_name;\n",
    "\n",
    "Retrieve total revenue for each customer:\n",
    "SELECT\n",
    "    c.customer_id,\n",
    "    c.customer_name,\n",
    "    SUM(s.revenue) AS total_revenue\n",
    "FROM\n",
    "    SalesFact s\n",
    "    INNER JOIN CustomerDimension c ON s.customer_id = c.customer_id\n",
    "GROUP BY\n",
    "    c.customer_id, c.customer_name;\n",
    "\n",
    "\n",
    "\n",
    "Retrieve total revenue for each product category in a specific year:\n",
    "SELECT\n",
    "    p.category,\n",
    "    SUM(s.revenue) AS total_revenue\n",
    "FROM\n",
    "    SalesFact s\n",
    "    INNER JOIN ProductDimension p ON s.product_id = p.product_id\n",
    "    INNER JOIN TimeDimension t ON s.date_id = t.date_id\n",
    "WHERE\n",
    "    t.year = 2023 r\n",
    "GROUP BY\n",
    "    P.category;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae355ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Design an ETL process using a programming language (e.g., Python) to extract data from a source system (e.g., CSV files), transform it by applying certain business rules or calculations, and load it into a data warehouse.\n",
    "\n",
    "#Extract Data from CSV Files:\n",
    "import pandas as pd\n",
    "\n",
    "###### Define the path to the CSV files\n",
    "csv_file_path = r\"C:\\Users\\ADMIN\\Desktop\\\"\n",
    "\n",
    "# Read the CSV files into Pandas DataFrames\n",
    "df_products = pd.read_csv(csv_file_path + \"products.csv\")\n",
    "df_customers = pd.read_csv(csv_file_path + \"customers.csv\")\n",
    "df_sales = pd.read_csv(csv_file_path + \"sales.csv\")\n",
    "\n",
    "#Transform Data:\n",
    "# Add a new column to calculate total revenue for each sales record\n",
    "df_sales['total_revenue'] = df_sales['quantity'] * df_sales['price']\n",
    "\n",
    "# Merge the DataFrames to combine relevant information\n",
    "df_merged = pd.merge(df_sales, df_products, on='product_id', how='inner')\n",
    "df_merged = pd.merge(df_merged, df_customers, on='customer_id', how='inner')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_transformed = df_merged.drop(['price'], axis=1)\n",
    "\n",
    "# Rename columns if necessary\n",
    "df_transformed = df_transformed.rename(columns={'product_name': 'product', 'customer_name': 'customer'})\n",
    "\n",
    "\n",
    "#Load Data into the Data Warehouse:\n",
    "\n",
    "import sqlalchemy\n",
    "\n",
    "# Define the connection string to the data warehouse (replace with your own connection details)\n",
    "database_connection = \"postgresql://username:password@localhost:5432/database_name\"\n",
    "\n",
    "# Establish a connection to the data warehouse\n",
    "engine = sqlalchemy.create_engine(database_connection)\n",
    "\n",
    "# Load the transformed data into the data warehouse\n",
    "df_transformed.to_sql('sales_fact', engine, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176cbcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Implement the ETL process by writing code that performs the extraction, transformation, and loading steps.\n",
    "\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "# Define the path to the CSV files\n",
    "csv_file_path = r\"C:\\Users\\ADMIN\\Desktop\\\"\n",
    "\n",
    "database_connection = \"postgresql://username:password@localhost:5432/database_name\"\n",
    "\n",
    "# Extract Data from CSV Files\n",
    "df_products = pd.read_csv(csv_file_path + \"products.csv\")\n",
    "df_customers = pd.read_csv(csv_file_path + \"customers.csv\")\n",
    "df_sales = pd.read_csv(csv_file_path + \"sales.csv\")\n",
    "\n",
    "# Transform Data\n",
    "df_sales['total_revenue'] = df_sales['quantity'] * df_sales['price']\n",
    "df_merged = pd.merge(df_sales, df_products, on='product_id', how='inner')\n",
    "df_merged = pd.merge(df_merged, df_customers, on='customer_id', how='inner')\n",
    "df_transformed = df_merged.drop(['price'], axis=1)\n",
    "df_transformed = df_transformed.rename(columns={'product_name': 'product', 'customer_name': 'customer'})\n",
    "\n",
    "# Load Data into the Data Warehouse\n",
    "engine = sqlalchemy.create_engine(database_connection)\n",
    "df_transformed.to_sql('sales_fact', engine, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f590bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC: Dimensional Modeling and Schemas\n",
    "   1. Design a star schema for a university database, including a fact table for student enrollments and dimension tables for students, courses, and time. Implement the schema using a database of your choice.\n",
    "\n",
    "Schema Design:\n",
    "Student Dimension Table:\n",
    "    student_id (Primary Key)\n",
    "    student_name\n",
    "    student_age\n",
    "    student_major\n",
    "Course Dimension Table:\n",
    "    course_id (Primary Key)\n",
    "    course_name\n",
    "    course_department\n",
    "    course_credits\n",
    "Time Dimension Table:\n",
    "    date_id (Primary Key)\n",
    "    date\n",
    "    day_of_week\n",
    "    month\n",
    "    quarter\n",
    "    year\n",
    "Enrollment Fact Table:\n",
    "    enrollment_id (Primary Key)\n",
    "    student_id (Foreign Key referencing Student Dimension Table)\n",
    "    course_id (Foreign Key referencing Course Dimension Table)\n",
    "    date_id (Foreign Key referencing Time Dimension Table)\n",
    "    grade\n",
    "\n",
    "CREATE TABLE StudentDimension (\n",
    "    student_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    student_name VARCHAR(255),\n",
    "    student_age INT,\n",
    "    student_major VARCHAR(50)\n",
    ");\n",
    "\n",
    "CREATE TABLE CourseDimension (\n",
    "    course_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    course_name VARCHAR(255),\n",
    "    course_department VARCHAR(50),\n",
    "    course_credits INT\n",
    ");\n",
    "\n",
    "CREATE TABLE TimeDimension (\n",
    "    date_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    date DATE,\n",
    "    day_of_week VARCHAR(20),\n",
    "    month VARCHAR(20),\n",
    "    quarter VARCHAR(20),\n",
    "    year INT\n",
    ");\n",
    "\n",
    "CREATE TABLE EnrollmentFact (\n",
    "    enrollment_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    student_id INT FOREIGN KEY REFERENCES StudentDimension(student_id),\n",
    "    course_id INT FOREIGN KEY REFERENCES CourseDimension(course_id),\n",
    "    date_id INT FOREIGN KEY REFERENCES TimeDimension(date_id),\n",
    "    grade VARCHAR(2)\n",
    ");\n",
    "\n",
    "   2. Write SQL queries to retrieve data from the star schema, including aggregations and joins between the fact table and dimension tables.\n",
    "\n",
    "--Retrieve total enrollments for each course:\n",
    "SELECT\n",
    "    cd.course_id,\n",
    "    cd.course_name,\n",
    "    COUNT(*) AS total_enrollments\n",
    "FROM\n",
    "    EnrollmentFact ef\n",
    "    INNER JOIN CourseDimension cd ON ef.course_id = cd.course_id\n",
    "GROUP BY\n",
    "    cd.course_id, cd.course_name;\n",
    "\n",
    "--Retrieve the average grade for each student:\n",
    "SELECT\n",
    "    sd.student_id,\n",
    "    sd.student_name,\n",
    "    AVG(CAST(ef.grade AS FLOAT)) AS average_grade\n",
    "FROM\n",
    "    EnrollmentFact ef\n",
    "    INNER JOIN StudentDimension sd ON ef.student_id = sd.student_id\n",
    "GROUP BY\n",
    "    sd.student_id, sd.student_name;\n",
    "\n",
    "\n",
    "--Retrieve the number of enrollments by quarter and year:\n",
    "SELECT\n",
    "    td.quarter,\n",
    "    td.year,\n",
    "    COUNT(*) AS enrollments_count\n",
    "FROM\n",
    "    EnrollmentFact ef\n",
    "    INNER JOIN TimeDimension td ON ef.date_id = td.date_id\n",
    "GROUP BY\n",
    "    td.quarter, td.year;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f806d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC: Performance Optimization and Querying\n",
    "    1. Scenario: You need to improve the performance of your data loading process in the data warehouse. Write a Python script that implements the following optimizations:\n",
    "Utilize batch processing techniques to load data in bulk instead of individual row insertion.\n",
    "      b)  Implement multi-threading or multiprocessing to parallelize the data loading process.\n",
    "      c)  Measure the time taken to load a specific amount of data before and after implementing these optimizations.\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import sqlalchemy\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_file_path = \"path/to/csv/file.csv\"\n",
    "\n",
    "# Define the connection string to the data warehouse (replace with your own connection details)\n",
    "database_connection = \"postgresql://username:password@localhost:5432/database_name\"\n",
    "\n",
    "# Define the batch size for batch processing\n",
    "batch_size = 1000\n",
    "\n",
    "# Function to load data in batches\n",
    "def load_data_in_batches(data, table_name):\n",
    "    engine = sqlalchemy.create_engine(database_connection)\n",
    "    with engine.begin() as connection:\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            batch_data.to_sql(table_name, connection, if_exists='append', index=False)\n",
    "\n",
    "# Function to load data using multi-threading\n",
    "def load_data_with_multithreading(data, table_name, num_threads):\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            future = executor.submit(load_data_in_batches, batch_data, table_name)\n",
    "            futures.append(future)\n",
    "        # Wait for all threads to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "# Function to load data using multiprocessing\n",
    "def load_data_with_multiprocessing(data, table_name, num_processes):\n",
    "    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
    "        futures = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            future = executor.submit(load_data_in_batches, batch_data, table_name)\n",
    "            futures.append(future)\n",
    "        # Wait for all processes to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "# Measure the time taken to load data\n",
    "def measure_loading_time(data, table_name):\n",
    "    start_time = time.time()\n",
    "    load_data_in_batches(data, table_name)\n",
    "    end_time = time.time()\n",
    "    loading_time = end_time - start_time\n",
    "    print(f\"Time taken to load data without optimization: {loading_time} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    load_data_with_multithreading(data, table_name, num_threads=4)\n",
    "    end_time = time.time()\n",
    "    loading_time = end_time - start_time\n",
    "    print(f\"Time taken to load data with multi-threading: {loading_time} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    load_data_with_multiprocessing(data, table_name, num_processes=4)\n",
    "    end_time = time.time()\n",
    "    loading_time = end_time - start_time\n",
    "    print(f\"Time taken to load data with multiprocessing: {loading_time} seconds\")\n",
    "\n",
    "# Read the data from the CSV file\n",
    "df_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Measure the time taken to load data before and after optimization\n",
    "measure_loading_time(df_data, \"sales_fact\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
